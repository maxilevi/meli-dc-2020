{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import helpers\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "FACTORS = 100\n",
    "EPOCHS = 20\n",
    "SEARCH_WEIGHT = 0.15 / 5\n",
    "KAGGLE = False\n",
    "MODEL_TYPE = 'rankfm'#rankfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items loaded!\n",
      "Interactions loaded!\n"
     ]
    }
   ],
   "source": [
    "items = helpers.load_items_df()\n",
    "items_dict = helpers.load_items()\n",
    "domain_item_dict = helpers.load_domain_item_dict(items_dict)\n",
    "all_items = list(items_dict.keys())\n",
    "print(\"Items loaded!\")\n",
    "interactions_train = helpers.load_interactions_df()\n",
    "if TEST:\n",
    "    interactions_test = helpers.load_interactions_test_df()\n",
    "print(\"Interactions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_times = collections.Counter(interactions_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-10-19T11:25:42.444-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-10-20T19:28:41.646-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-10-20T19:28:14.619-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-10-20T10:37:47.699-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-10-20T10:37:23.202-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "      <td>search</td>\n",
       "      <td>2019-09-25T08:41:34.424-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>MLB-KIDS_TENTS</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-09-25T16:16:36.102-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>MLB-TOYS_AND_GAMES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-09-25T16:09:22.866-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>MLB-TOYS_AND_GAMES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-09-25T16:09:42.478-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>MLB-TOYS_AND_GAMES</td>\n",
       "      <td>view</td>\n",
       "      <td>2019-09-25T16:09:52.953-0400</td>\n",
       "      <td>MLB-SMARTWATCHES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id             item_id event_type               event_timestamp  \\\n",
       "0         0    MLB-SMARTWATCHES       view  2019-10-19T11:25:42.444-0400   \n",
       "1         0    MLB-SMARTWATCHES       view  2019-10-20T19:28:41.646-0400   \n",
       "2         0    MLB-SMARTWATCHES       view  2019-10-20T19:28:14.619-0400   \n",
       "3         0    MLB-SMARTWATCHES       view  2019-10-20T10:37:47.699-0400   \n",
       "4         0    MLB-SMARTWATCHES       view  2019-10-20T10:37:23.202-0400   \n",
       "..      ...                 ...        ...                           ...   \n",
       "95        4    MLB-SMARTWATCHES     search  2019-09-25T08:41:34.424-0400   \n",
       "96        4      MLB-KIDS_TENTS       view  2019-09-25T16:16:36.102-0400   \n",
       "97        4  MLB-TOYS_AND_GAMES       view  2019-09-25T16:09:22.866-0400   \n",
       "98        4  MLB-TOYS_AND_GAMES       view  2019-09-25T16:09:42.478-0400   \n",
       "99        4  MLB-TOYS_AND_GAMES       view  2019-09-25T16:09:52.953-0400   \n",
       "\n",
       "              target  \n",
       "0   MLB-SMARTWATCHES  \n",
       "1   MLB-SMARTWATCHES  \n",
       "2   MLB-SMARTWATCHES  \n",
       "3   MLB-SMARTWATCHES  \n",
       "4   MLB-SMARTWATCHES  \n",
       "..               ...  \n",
       "95  MLB-SMARTWATCHES  \n",
       "96  MLB-SMARTWATCHES  \n",
       "97  MLB-SMARTWATCHES  \n",
       "98  MLB-SMARTWATCHES  \n",
       "99  MLB-SMARTWATCHES  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(row):\n",
    "    nan = float('nan')\n",
    "    target = int(row['target'])\n",
    "    itemid = int(row['item_id'])\n",
    "    row['item_id'] = items_dict[int(row['item_id'])]['domain_id'] if (type(row['item_id']) != float or not math.isnan(row['item_id'])) else nan\n",
    "    row['target'] = items_dict[int(row['target'])]['domain_id']\n",
    "    return row\n",
    "interactions_train.head(100).apply(process, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_item_features(items, interactions):\n",
    "    viewed_times = collections.Counter(interactions[interactions['event_type'] == 'view'].item_id)\n",
    "    items_df = items[['item_id', 'domain_id', 'price', 'condition']].copy()\n",
    "    #items_df = pd.get_dummies(items_df, columns=['domain_id', 'condition'])\n",
    "    '''\n",
    "    domains = items.domain_id.unique() \n",
    "    m = int(math.log2(len(domains)) + 1)\n",
    "    columns = {f'domain_bit_{i}': [] for i in range(m)}\n",
    "    indexed_domains = {domains[i]: i for i in range(len(domains))}\n",
    "\n",
    "    def domain_apply(x):\n",
    "        arr = helpers.bin_array(indexed_domains[x], m)\n",
    "        for j in range(m):\n",
    "            columns[f'domain_bit_{j}'].append(arr[j])\n",
    "\n",
    "    items_df['domain_id'].apply(domain_apply)\n",
    "'''\n",
    "    #for k in columns.keys():\n",
    "    #    items_df[k] = columns[k]\n",
    "    #le = LabelEncoder()\n",
    "    \n",
    "    #items_df['condition'] = items_df['condition'].apply(lambda x: 1 if x == 'new' else 0)\n",
    "    #items_df['sold_times'] = items_df['item_id'].apply(lambda x: sold_times[x])\n",
    "    items_df['viewed_times'] = items_df['item_id'].apply(lambda x: viewed_times[x])\n",
    "    #items_df['domain_id'] = le.fit_transform(items_df['domain_id'])\n",
    "    #items_df = items_df.drop(columns=['domain_id'])\n",
    "    #items_df['item_id'] = items_df['item_id'].astype(int)\n",
    "    #items_df['price'] = items_df['price'].fillna(0)\n",
    "    #scaler = MinMaxScaler()\n",
    "    #transformed_price = scaler.fit_transform(items_df['price'].values.reshape(-1, 1)).flatten()\n",
    "    #items_df['price'] = pd.Series(transformed_price)\n",
    "    return items_df\n",
    "\n",
    "def encode_user_features(users, interactions):\n",
    "    event_dict = dict(list(interactions.groupby('user_id')))\n",
    "    data = {'user_id': [], 'items_viewed': [], 'searches_done': [], 'categories_viewed': []}\n",
    "    for u in users:\n",
    "        events = event_dict[u].values.tolist()\n",
    "        items_viewed = 0\n",
    "        categories_viewed = set()\n",
    "        searches_done = 0\n",
    "        for event in events:\n",
    "            user_id, info, event_type, timestamp, target = event\n",
    "            if np.isnan(info): \n",
    "                continue\n",
    "            \n",
    "            if event_type == 'search':\n",
    "                searches_done += 1\n",
    "            else:\n",
    "                items_viewed += 1\n",
    "            \n",
    "            categories_viewed.add(items_dict[info]['domain_id'])\n",
    "            \n",
    "        data['user_id'].append(u)\n",
    "        data['items_viewed'].append(items_viewed)\n",
    "        data['searches_done'].append(searches_done)\n",
    "        data['categories_viewed'].append(len(categories_viewed))\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_interactions(df):\n",
    "    new_df = df[pd.notnull(df['item_id'])].copy()\n",
    "    new_df['user_id'] = new_df['user_id'].astype(float).astype(int)\n",
    "    new_df['item_id'] = new_df['item_id'].astype(float).astype(int)\n",
    "    sample_weights = np.array([(3 if x != 'search' else SEARCH_WEIGHT) for x in new_df['event_type']])\n",
    "    return new_df[['user_id', 'item_id']], sample_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_candidate_pairs(users, valid_item_ids):\n",
    "    users_column = []\n",
    "    items_column = []\n",
    "    user_lengths = []\n",
    "    i = 0\n",
    "    for u in users:\n",
    "        candidates = [x for x in get_candidates(u) if x in valid_item_ids]\n",
    "        items_column += candidates\n",
    "        users_column += [u] * len(candidates)\n",
    "        user_lengths.append((u, len(candidates)))\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Progress {i}/{len(users)}\")\n",
    "        i += 1\n",
    "    pairs = pd.DataFrame({'user_id': users_column, 'item_id': items_column})\n",
    "    return pairs, users_column, items_column, user_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_recommendations(recommendations_pairs, items_column, user_lengths):\n",
    "    offset = 0\n",
    "    recommendations = {}\n",
    "    for user, user_len in user_lengths:\n",
    "        user_recs = recommendations_pairs[offset:offset+user_len]\n",
    "        ranked_recs = np.argsort(user_recs)[::-1]\n",
    "        top_10 = [x for x in ranked_recs if not np.isnan(user_recs[x])][:10]\n",
    "        recommendations[user] = [items_column[x + offset] for x in top_10]\n",
    "        offset += user_len\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domains_from_items(items):\n",
    "    return set(items_dict[int(item)]['domain_id'] for item in items)\n",
    "\n",
    "def get_candidates(user):\n",
    "    items_interacted = event_dict[user] if user in event_dict else set()\n",
    "    domains = get_domains_from_items(items_interacted) if items_interacted else top_domains[:10]\n",
    "    items_for_domains = [domain_top_items[d] for d in domains]\n",
    "    item_universe = sum(items_for_domains, [])\n",
    "        \n",
    "    for item in item_universe:\n",
    "        items_interacted.add(item)\n",
    "            \n",
    "    return list(items_interacted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_interactions(i1, i2):\n",
    "    i1c = i1.copy()\n",
    "    i2c = i2.copy()\n",
    "    i2c['user_id'] += i1c.shape[0]\n",
    "    return i1c.append(i2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 5.41 s, total: 45.2 s\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "users = None\n",
    "interactions = None\n",
    "sample_weights = None\n",
    "user_features = None\n",
    "\n",
    "if TEST:\n",
    "    interactions = (combine_interactions(interactions_train, interactions_test))#shuffle\n",
    "    validation_users = interactions_test.user_id.unique() + interactions_train.shape[0]\n",
    "    all_users = np.concatenate([interactions_train.user_id.unique(), validation_users])\n",
    "else:\n",
    "    interactions = (interactions_train)\n",
    "    validation_users = interactions_train.user_id.unique()\n",
    "    all_users = validation_users\n",
    "\n",
    "user_target_dict = None\n",
    "#interactions = interactions[interactions['event_type'] != 'search']\n",
    "\n",
    "## Calculate user features\n",
    "#interactions_users = set(interactions.user_id.dropna().unique())\n",
    "#%time user_features = encode_user_features(interactions_users, interactions)\n",
    "\n",
    "## Calculate item features\n",
    "#interactions_items = set(interactions.item_id.dropna().unique())\n",
    "#items_cp = items.copy()\n",
    "#items_cp.set_index('item_id', inplace=True, drop=False)\n",
    "#%time item_features = encode_item_features(items_cp.loc[interactions_items], interactions)\n",
    "#item_features = item_features.reset_index(drop=True)\n",
    "\n",
    "## Calculate auxiliary data\n",
    "interactions, sample_weights = encode_interactions(interactions)\n",
    "domain_top_items = helpers.load_top_items(interactions_train, domain_item_dict)\n",
    "top_domains = helpers.load_top_domains(interactions_train, domain_top_items)\n",
    "event_dict = interactions.groupby('user_id')['item_id'].unique().apply(set).to_dict()\n",
    "valid_item_ids = set(interactions['item_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    !pip install rankfm\n",
    "    !pip install lightfM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    from lightfm.data import Dataset\n",
    "    from lightfm import LightFM\n",
    "    from lightfm.data import Dataset\n",
    "    from scipy.sparse import coo_matrix\n",
    "    import scipy\n",
    "\n",
    "    item_feature_values = set()\n",
    "    for column in item_features.columns:\n",
    "        if column == 'item_id': continue\n",
    "        item_feature_values |= set(item_features[column].unique())\n",
    "\n",
    "    user_feature_values = set()\n",
    "    for column in user_features.columns:\n",
    "        if column == 'user_id': continue\n",
    "        user_feature_values |= set(user_features[column].unique())\n",
    "\n",
    "    item_ids = interactions['item_id'].unique()\n",
    "    user_ids = all_users\n",
    "\n",
    "    dataset = Dataset()\n",
    "    dataset.fit(\n",
    "        user_ids,\n",
    "        item_ids,\n",
    "        item_features=item_feature_values,\n",
    "        user_features=user_feature_values\n",
    "    )\n",
    "\n",
    "    train_interactions, train_weights = dataset.build_interactions(\n",
    "        ((x[1], x[2], sample_weights[i]) for i, x in enumerate(interactions.itertuples())),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    lightfm_item_features = dataset.build_item_features(\n",
    "        item_features.apply(lambda x: (x['item_id'], [x[y] for y in x.index[1:] if y != 'item_id']), axis=1).values,\n",
    "    )\n",
    "    lightfm_user_features = dataset.build_user_features(\n",
    "        user_features.apply(lambda x: (x['user_id'], [x[y] for y in x.index[1:] if y != 'user_id']), axis=1).values,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ITEM_ALPHA = 1e-6\n",
    "if MODEL_TYPE == 'lightfm':\n",
    "    \n",
    "    print(f'Training {train_interactions.shape[0]} samples for {EPOCHS} epochs')\n",
    "    model = LightFM(\n",
    "        no_components=25,\n",
    "        loss='warp',\n",
    "        item_alpha=ITEM_ALPHA\n",
    "    )\n",
    "    ##\n",
    "    ## Try no normalization\n",
    "    ## Try changing the get_candidates() method\n",
    "    ## Do a submit\n",
    "    ## Try using searches to find the domains\n",
    "    ##\n",
    "    model.fit(\n",
    "        train_interactions,\n",
    "        epochs=20,\n",
    "        #sample_weight=train_weights,\n",
    "        #item_features=lightfm_item_features,\n",
    "        #user_features=lightfm_user_features,\n",
    "        num_threads=4,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"Building recommendation pairs...\")\n",
    "    pairs, users_column, items_column, user_lengths = build_candidate_pairs(validation_users, valid_item_ids)\n",
    "\n",
    "    user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()\n",
    "    print(f\"Generating recommendation pairs\")\n",
    "    recommendations_pairs = model.predict(\n",
    "        np.array([user_id_map[x] for x in users_column]),#user_le.transform(users_column),\n",
    "        np.array([item_id_map[x] for x in items_column]),#item_le.transform(items_column),\n",
    "        #item_features=lightfm_item_features,\n",
    "        #user_features=lightfm_user_features,\n",
    "        num_threads=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RankFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 11999164 interactions...\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n",
      "\n",
      "training epoch: 0\n",
      "log likelihood: -5387343.0\n",
      "\n",
      "training epoch: 1\n",
      "log likelihood: -3077177.5\n",
      "\n",
      "training epoch: 2\n",
      "log likelihood: -1921957.125\n",
      "\n",
      "training epoch: 3\n",
      "log likelihood: -1345258.25\n",
      "\n",
      "training epoch: 4\n",
      "log likelihood: -1049952.0\n",
      "\n",
      "training epoch: 5\n",
      "log likelihood: -878200.0\n",
      "\n",
      "training epoch: 6\n",
      "log likelihood: -775879.8125\n",
      "\n",
      "training epoch: 7\n",
      "log likelihood: -710139.0625\n",
      "\n",
      "training epoch: 8\n",
      "log likelihood: -665708.6875\n",
      "\n",
      "training epoch: 9\n",
      "log likelihood: -634133.5625\n",
      "\n",
      "training epoch: 10\n",
      "log likelihood: -609428.0\n",
      "\n",
      "training epoch: 11\n",
      "log likelihood: -589565.5625\n",
      "\n",
      "training epoch: 12\n",
      "log likelihood: -573660.25\n",
      "\n",
      "training epoch: 13\n",
      "log likelihood: -560414.625\n",
      "\n",
      "training epoch: 14\n",
      "log likelihood: -549603.0625\n",
      "\n",
      "training epoch: 15\n",
      "log likelihood: -540292.3125\n",
      "\n",
      "training epoch: 16\n",
      "log likelihood: -532446.0625\n",
      "\n",
      "training epoch: 17\n",
      "log likelihood: -525471.125\n",
      "\n",
      "training epoch: 18\n",
      "log likelihood: -519403.0625\n",
      "\n",
      "training epoch: 19\n",
      "log likelihood: -514034.03125\n",
      "\n",
      "training epoch: 20\n",
      "log likelihood: -509358.8125\n",
      "\n",
      "training epoch: 21\n",
      "log likelihood: -504956.03125\n",
      "\n",
      "training epoch: 22\n",
      "log likelihood: -501109.46875\n",
      "\n",
      "training epoch: 23\n",
      "log likelihood: -497645.9375\n",
      "\n",
      "training epoch: 24\n",
      "log likelihood: -494348.53125\n",
      "CPU times: user 42min 14s, sys: 53.2 s, total: 43min 8s\n",
      "Wall time: 44min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MODEL_TYPE == 'rankfm':\n",
    "    from rankfm.rankfm import RankFM\n",
    "    \n",
    "    model = RankFM(factors=FACTORS, loss='warp', max_samples=20, alpha=0.01, sigma=0.1, learning_rate=0.10, learning_schedule='invscaling')\n",
    "    \n",
    "    print(f\"Fitting {interactions.shape[0]} interactions...\")\n",
    "    \n",
    "    %time\n",
    "    model.fit(\n",
    "        interactions,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=True,\n",
    "        sample_weight=sample_weights,\n",
    "        #item_features=item_features,\n",
    "        #user_features=user_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'./data/model_(e=25, t=False, sw=3, vw=3, t=st)2.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating candidate pairs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 134 µs\n",
      "Progress 0/413163\n",
      "Progress 100000/413163\n",
      "Progress 200000/413163\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating candidate pairs\")\n",
    "\n",
    "%time\n",
    "pairs, users_column, items_column, user_lengths = build_candidate_pairs(validation_users, valid_item_ids)\n",
    "\n",
    "print(f\"Generating recommnedation pairs\")\n",
    "\n",
    "%time\n",
    "recommendations_pairs = model.predict(pairs, cold_start='nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create item users pairs to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommendations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a7246b5ec68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Assert required sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommendations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0munfilled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommendations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munfilled\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recommendations' is not defined"
     ]
    }
   ],
   "source": [
    "def fill(recommendations):\n",
    "    for k in recommendations.keys():\n",
    "        if len(recommendations[k]) == 0:\n",
    "            recommendations[k] = random.choices(all_items, k=10)\n",
    "        elif len(recommendations[k]) < 10:\n",
    "            category = items_dict[recommendations[k][0]]['domain_id']\n",
    "            recommendations[k] += random.choices(domain_item_dict[category], k=(10 - len(recommendations[k])))\n",
    "\n",
    "# Assert required sizes\n",
    "            \n",
    "assert len(recommendations) == len(validation_users)\n",
    "unfilled = len([True for k in recommendations.keys() if len(recommendations[k]) != 10])\n",
    "if unfilled > 0:\n",
    "    print(f\"{unfilled} entries were not filled. Extending the items...\")\n",
    "    fill(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring (if training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST and not user_target_dict:\n",
    "    user_target_dict = interactions_train.groupby('user_id')['target'].unique().apply(lambda x: x).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relevance(items_dict, item, target):\n",
    "    if item == target:\n",
    "        return 15\n",
    "    if items_dict[item]['domain_id'] == items_dict[target]['domain_id']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def _get_perfect_dcg():\n",
    "    perfect = [15, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    return sum(perfect[i] / np.log2(i + 2) for i in range(len(perfect))) / len(perfect)\n",
    "\n",
    "def _dcg(items_dict, recommendations, target):\n",
    "    \n",
    "    dcg = sum(_relevance(items_dict, recommendations[i], target) / np.log2(i + 2) for i in range(len(recommendations)))\n",
    "    return dcg / len(recommendations)\n",
    "\n",
    "def ndcg_score(items_dict, recommendations, user_targets_dict):\n",
    "    sum_ndcg = 0\n",
    "    sum_perfect = 0\n",
    "    for x in recommendations.keys():\n",
    "        sum_ndcg += _dcg(items_dict, [int(w) for w in recommendations[x]], int(user_targets_dict[x]))\n",
    "        sum_perfect += _get_perfect_dcg()\n",
    "\n",
    "    return sum_ndcg / sum_perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST:\n",
    "    print(ndcg_score(items_dict, recommendations, user_target_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.25357310893765944\n",
    "#0.25013763660310895\n",
    "#train with search w=0.1 factors=250 item_features=no -> 0.243\n",
    "#train with search w=0.2 factors=150 item_features=no -> 0.24443886619434535\n",
    "#train with search w=0.5 factors=250 item_features=no -> 0.23223811466616529\n",
    "#train with search w=1.0 factors=250 item_features=no -> 0.19637033782036525\n",
    "#train with search w=1.0 factors=100 item_features=no -> 0.197764356297992\n",
    "#train with search w=0.15 factors=100 item_features=no ->0.24546062804586635\n",
    "\n",
    "# lightfm n=50 no search -> 0.2481555866759734\n",
    "# lightfm n=25 -> 0.24950549809750702\n",
    "# lightfm n=25 w integer item features w item alpha -> 0.22521187808442616\n",
    "# lightfm n=25 -> 0.25071840818293245\n",
    "# lightfm n=25 w item alpha -> 0.2508261003278925\n",
    "\n",
    "#lightfm n=25 w search w weights-> 0.2191484993506384\n",
    "\n",
    "# only domain ~0.07660177371164308\n",
    "# only target ~0.16885885433\n",
    "# lightfm only domain with item_f = 0.08535225261531444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating submit (if testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    submit = pd.DataFrame(recommendations)\n",
    "    print(f'Submit shape is {submit.shape}')\n",
    "    assert submit.shape == (10, 177070)\n",
    "    submit.transpose().to_csv(f'submit_f={FACTORS}_e={EPOCHS}.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1487e02af5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mTEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mrecommendations_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST' is not defined"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    import pickle\n",
    "    with open(f\"./data/recommendations/E={epochs}\", \"wb\") as f:\n",
    "        pickle.dump(recommendations_pairs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding domains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
